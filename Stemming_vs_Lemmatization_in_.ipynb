{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z10H93dYeGo7"
      },
      "outputs": [],
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iCgs3UR9eL1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming vs Lemmatization in\n",
        "\n",
        "What is Stemming in NLP?\n",
        "\n",
        "Stemming is the process of removing the last few characters of a given word, to obtain a shorter form, even if that form doesn’t have any meaning.\n",
        "\n",
        "Why we Need Stemming?\n",
        "\n",
        "Code for Stemming Explained\n"
      ],
      "metadata": {
        "id": "EN2dccOkeNPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "rDfxbpZOeQMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYYfs9vMeehU",
        "outputId": "e531efd1-2465-4bbf-b70f-e3595b5d32fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "    I have three visions for India. In 3000 years of our history,\n",
        "    people from all over the world have come and invaded us, captured our  lands, conquered our minds.\n",
        "    From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "    the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "    Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "    We have not grabbed their land, their culture,\n",
        "    their history and tried to enforce our way of life on them.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "aQaILS52esKa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization (step before stemming)\n",
        "\n",
        "Before, stemming, tokenization is done so as to break text into chunks. In this case, paragraph to sentences for easy computation.\n",
        "\n",
        "As can be seen from output paragraph is divided into sentences based on “.” .\n"
      ],
      "metadata": {
        "id": "DqlAiOwKfPzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n",
        "\n",
        "In the code given below, one sentence is taken at a time and word tokenization is applied i.e., converting sentence to words. After that, stopwords (such as the, and, etc) are ignored and stemming is applied on all other words. Finally, stem words are joined to make a sentence.\n",
        "\n",
        "Note: Stopwords are the words that do not add any value to the sentence.\n",
        "\n",
        "Python Code:"
      ],
      "metadata": {
        "id": "VjZpeLcKffxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkpp_rAYfhaf",
        "outputId": "f33fa686-3d4c-4712-e9a0-cb8a689b2065"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over  the world have come and invaded us, captured our lands, conquered our minds.  From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.  Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.  \"\"\"\n"
      ],
      "metadata": {
        "id": "6Dg53XsKfs0g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wKwUbPff4O_",
        "outputId": "95826a27-23b6-41fc-d20a-ae87b78aeb84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I have three visions for India.', 'In 3000 years of our history, people from all over  the world have come and invaded us, captured our lands, conquered our minds.', 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.', 'Yet we have not done this to any other nation.', 'We have not conquered anyone.', 'We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n Result after Stemming \\n\\n\")\n",
        "stemmer = nltk.PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6H7PTzf9ME",
        "outputId": "24da30e0-2ba7-4671-f2ee-d22c3b008bf6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Result after Stemming \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JAQ6lo4gEYz",
        "outputId": "e467102d-384c-4e7c-f028-62642632c38a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['three vision india .', '3000 year histori , peopl world come invad us , captur land , conquer mind .', 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .', 'yet done nation .', 'conquer anyon .', 'grab land , cultur , histori tri enforc way life .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output, we can see that stopwords such as have, for have been removed from sentence one. The word “visions” have been converted to “vision, “history” to “histori” by stemming."
      ],
      "metadata": {
        "id": "BKjZfGfXgb7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Lemmatization Explained"
      ],
      "metadata": {
        "id": "7ToZPO3AiQAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1CS5nAWiRF4",
        "outputId": "81eee654-591a-4c66-a5f5-e6d940a9605f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               \"\"\""
      ],
      "metadata": {
        "id": "pahhVgxlidAP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization (step before stemming)"
      ],
      "metadata": {
        "id": "KJLK2tctinP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "jbgMC2hjioat",
        "outputId": "32c6773d-4c55-423e-fb66-900eeb8191f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I have three visions for India.', 'In 3000 years of our history, people from all over\\n               the world have come and invaded us, captured our lands, conquered our minds.', 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.', 'Yet we have not done this to any other nation.', 'We have not conquered anyone.', 'We have not grabbed their land, their culture,\\n               their history and tried to enforce our way of life on them.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization\n",
        "\n",
        "The difference between stemming and lemmatization comes in this step where WordNetLemmatizer() is used instead of PorterStemmer(). Rest of steps are the same."
      ],
      "metadata": {
        "id": "jSi2yEmdq_18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bRkxvRROrbJT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArcNb51Xr2e9",
        "outputId": "f6d38b6e-2018-4c1d-c011-788f82317c96"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I three vision India .', 'In 3000 year history , people world come invaded u , captured land , conquered mind .', 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .', 'Yet done nation .', 'We conquered anyone .', 'We grabbed land , culture , history tried enforce way life .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In above output, it can be noticed that although word “visions” have been converted to “vision” but word “history” remained “history” unlike stemming and thus retained its meaning."
      ],
      "metadata": {
        "id": "zw75WmUXsihU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hxHrdfUjs0yW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6Svgqqhs3ni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}